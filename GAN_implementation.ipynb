{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGUzz4Rb-_Yj",
        "outputId": "c76c066c-fe79-4945-8d37-88451b997f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEBHvk8d9efe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def load_images(folder_path, target_res):\n",
        "    images = []\n",
        "    resolutions = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(folder_path, filename)\n",
        "            img = Image.open(img_path)\n",
        "            current_res = min(img.size)  # Use the smaller dimension as resolution\n",
        "            resolutions.append(current_res)\n",
        "\n",
        "            # Resize the image to target_res x target_res\n",
        "            img = img.resize((target_res, target_res))\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])\n",
        "            img_tensor = transform(img)\n",
        "            images.append(img_tensor)\n",
        "\n",
        "    return torch.stack(images), resolutions\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(Generator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels + 2, 64, kernel_size=9, padding=4)  # +2 for resolution info\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1)\n",
        "        self.conv3 = nn.Conv2d(32, output_channels, kernel_size=5, padding=2)\n",
        "\n",
        "    def forward(self, x, current_res, target_res):\n",
        "        # Create resolution channels\n",
        "        batch_size, _, height, width = x.shape\n",
        "        current_res_channel = torch.full((batch_size, 1, height, width), current_res, device=x.device) / 255.0\n",
        "        target_res_channel = torch.full((batch_size, 1, height, width), target_res, device=x.device) / 255.0\n",
        "\n",
        "        # Concatenate input with resolution information\n",
        "        x = torch.cat([x, current_res_channel, target_res_channel], dim=1)\n",
        "\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        return torch.tanh(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
        "        # Calculate the correct input size for the fully connected layer\n",
        "        self.fc = nn.Linear(128 * (target_res // 4) * (target_res // 4), 1) # Changed this line to calculate the input size dynamically\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv4(x), 0.2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "AjiB0CQw_XlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate scale factor\n",
        "def calculate_scale_factor(current_res, target_res):\n",
        "    return target_res // current_res\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_gan(generator, discriminator, dataloader, resolutions, target_res, num_epochs, device):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (real_images, current_res) in enumerate(zip(dataloader, resolutions)):\n",
        "            batch_size = real_images.size(0)\n",
        "            real_images = real_images.to(device)\n",
        "            current_res = current_res.to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            optimizer_D.zero_grad()\n",
        "            real_labels = torch.ones(batch_size, 1).to(device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "            outputs = discriminator(real_images)\n",
        "            d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "            fake_images = generator(real_images, current_res, target_res)\n",
        "            outputs = discriminator(fake_images.detach())\n",
        "            d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Train Generator\n",
        "            optimizer_G.zero_grad()\n",
        "            outputs = discriminator(fake_images)\n",
        "            g_loss = criterion(outputs, real_labels)\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "emqBpOqR_bum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Get user input\n",
        "    folder_path = input(\"Enter the folder path containing images: \")\n",
        "    target_res = int(input(\"Enter the target resolution: \"))\n",
        "\n",
        "    # Load images and get resolutions\n",
        "    images, resolutions = load_images(folder_path, target_res)\n",
        "    dataloader = torch.utils.data.DataLoader(images, batch_size=4, shuffle=True)\n",
        "    resolutions = torch.tensor(resolutions)\n",
        "\n",
        "    # Initialize models\n",
        "    generator = Generator(3, 3).to(device)\n",
        "    discriminator = Discriminator(3).to(device)\n",
        "\n",
        "    # Train the GAN\n",
        "    num_epochs = 50\n",
        "    train_gan(generator, discriminator, dataloader, resolutions, target_res, num_epochs, device)\n",
        "\n",
        "    # Save the trained generator\n",
        "    torch.save(generator.state_dict(), 'generator.pth')\n",
        "\n",
        "    print(\"Training complete. Generator saved as 'generator.pth'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uktUVqee_fbt",
        "outputId": "9478718e-f7fa-4047-952d-1ffe4a4ff1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the folder path containing images: /content/drive/MyDrive/archive/samples/0001e96803--621ea6adb419e86592d408c5\n",
            "Enter the target resolution: 700\n",
            "Epoch [1/50], D_loss: 1.3866, G_loss: 0.3985\n",
            "Epoch [2/50], D_loss: 1.7406, G_loss: 4.8033\n",
            "Epoch [3/50], D_loss: 0.0388, G_loss: 9.0634\n",
            "Epoch [4/50], D_loss: 0.2367, G_loss: 9.2527\n",
            "Epoch [5/50], D_loss: 0.0026, G_loss: 8.4763\n",
            "Epoch [6/50], D_loss: 0.0006, G_loss: 7.2654\n",
            "Epoch [7/50], D_loss: 0.0021, G_loss: 5.8649\n",
            "Epoch [8/50], D_loss: 0.0100, G_loss: 5.2946\n",
            "Epoch [9/50], D_loss: 0.0258, G_loss: 6.5123\n",
            "Epoch [10/50], D_loss: 0.0145, G_loss: 8.4254\n",
            "Epoch [11/50], D_loss: 0.0035, G_loss: 10.1285\n",
            "Epoch [12/50], D_loss: 0.0013, G_loss: 10.8733\n",
            "Epoch [13/50], D_loss: 0.0041, G_loss: 10.7036\n",
            "Epoch [14/50], D_loss: 0.0154, G_loss: 13.4407\n",
            "Epoch [15/50], D_loss: 0.0007, G_loss: 16.8453\n",
            "Epoch [16/50], D_loss: 0.0000, G_loss: 20.5662\n",
            "Epoch [17/50], D_loss: 0.0000, G_loss: 25.0547\n",
            "Epoch [18/50], D_loss: 0.0000, G_loss: 30.5726\n",
            "Epoch [19/50], D_loss: 0.0000, G_loss: 36.8783\n",
            "Epoch [20/50], D_loss: 0.0000, G_loss: 43.3087\n",
            "Epoch [21/50], D_loss: 0.0000, G_loss: 49.5779\n",
            "Epoch [22/50], D_loss: 0.0000, G_loss: 55.3749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qZadeeFeAvy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}